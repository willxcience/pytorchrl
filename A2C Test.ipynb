{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchrl\n",
    "import torchrl.network as network\n",
    "from torchrl.a2c import A2C\n",
    "from torchrl.a2c import ActorCritic\n",
    "\n",
    "# later incorporate this into torchrl\n",
    "from torchrl.utils import make_atari_env\n",
    "from torchrl.utils import weights_init\n",
    "from torchrl.parallel import SubprocVecEnv\n",
    "from torchrl.parallel import VecFrameStack\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"num_in_channels\"] = 4\n",
    "params[\"num_latent_nodes\"] = 512\n",
    "\n",
    "# control the speed of training\n",
    "params[\"num_workers\"] = 10\n",
    "params[\"rollout_len\"] = 5\n",
    "params[\"use_cuda\"] = True\n",
    "\n",
    "# log interval steps\n",
    "params[\"log_interval\"] = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-3:\n",
      "Process Process-4:\n",
      "Process Process-2:\n",
      "Process Process-1:\n",
      "Process Process-9:\n",
      "Process Process-8:\n",
      "Process Process-10:\n",
      "Traceback (most recent call last):\n",
      "Process Process-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "Process Process-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/will/Desktop/pytorchrl/torchrl/parallel/subproc_vec_env.py\", line 14, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/will/Desktop/pytorchrl/torchrl/parallel/subproc_vec_env.py\", line 14, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/will/Desktop/pytorchrl/torchrl/parallel/subproc_vec_env.py\", line 14, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/will/Desktop/pytorchrl/torchrl/parallel/subproc_vec_env.py\", line 14, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/will/Desktop/pytorchrl/torchrl/parallel/subproc_vec_env.py\", line 14, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/will/Desktop/pytorchrl/torchrl/parallel/subproc_vec_env.py\", line 14, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/will/Desktop/pytorchrl/torchrl/parallel/subproc_vec_env.py\", line 14, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/will/Desktop/pytorchrl/torchrl/parallel/subproc_vec_env.py\", line 14, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/will/Desktop/pytorchrl/torchrl/parallel/subproc_vec_env.py\", line 14, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/will/Desktop/pytorchrl/torchrl/parallel/subproc_vec_env.py\", line 14, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/will/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "env = VecFrameStack(make_atari_env(\"BreakoutNoFrameskip-v4\", params[\"num_workers\"]), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"num_actions\"] = env.action_space.n\n",
    "\n",
    "# hyperparaters for traing\n",
    "params[\"discount_gamma\"] = 0.9\n",
    "params[\"entropy_coef\"] = 0.01\n",
    "params[\"value_coef\"] = 0.5\n",
    "params[\"use_gae\"] = True\n",
    "params[\"gae_tau\"] = 0.95\n",
    "params[\"max_grad_norm\"] = 0.5\n",
    "params[\"learning_rate\"] = 7e-4\n",
    "params[\"RMS_alpha\"] = 0.99\n",
    "params[\"RMS_eps\"] = 1e-5\n",
    "params[\"resume\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model and its optimizer\n",
    "model = ActorCritic(params)\n",
    "model.apply(weights_init)\n",
    "\n",
    "# RMSprop better for RL\n",
    "optimizer = optim.RMSprop(model.parameters(),\n",
    "                           params[\"learning_rate\"],\n",
    "                           alpha=params[\"RMS_alpha\"],\n",
    "                           eps=params[\"RMS_eps\"]\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = A2C(env, model, optimizer, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"resume\"]:\n",
    "    checkpoint = torch.load(\"Breakout/model0.pth\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    agent.num_steps = checkpoint[\"num_steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(agent.num_workers * agent.rollout_len)\n",
    "num_iter = int(4e7 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns 0.22/0.00/0.00/2.00 (mean/median/min/max)  670.13 steps/s\n",
      "Returns 0.20/0.00/0.00/2.00 (mean/median/min/max)  987.90 steps/s\n",
      "Returns 0.20/0.00/0.00/3.00 (mean/median/min/max)  1020.16 steps/s\n",
      "Returns 0.30/0.00/0.00/4.00 (mean/median/min/max)  1082.33 steps/s\n",
      "Returns 0.22/0.00/0.00/3.00 (mean/median/min/max)  1068.01 steps/s\n",
      "Returns 0.30/0.00/0.00/3.00 (mean/median/min/max)  1063.05 steps/s\n",
      "Returns 0.40/0.00/0.00/2.00 (mean/median/min/max)  1065.03 steps/s\n",
      "Returns 0.40/0.00/0.00/3.00 (mean/median/min/max)  1037.69 steps/s\n",
      "Returns 0.30/0.00/0.00/2.00 (mean/median/min/max)  1039.85 steps/s\n",
      "Returns 0.20/0.00/0.00/2.00 (mean/median/min/max)  1063.77 steps/s\n",
      "Returns 0.24/0.00/0.00/2.00 (mean/median/min/max)  1019.06 steps/s\n",
      "Returns 0.14/0.00/0.00/1.00 (mean/median/min/max)  978.74 steps/s\n",
      "Returns 0.38/0.00/0.00/2.00 (mean/median/min/max)  1021.25 steps/s\n",
      "Returns 0.28/0.00/0.00/3.00 (mean/median/min/max)  1044.36 steps/s\n",
      "Returns 0.14/0.00/0.00/3.00 (mean/median/min/max)  1036.94 steps/s\n",
      "Returns 0.16/0.00/0.00/3.00 (mean/median/min/max)  1066.66 steps/s\n",
      "Returns 0.18/0.00/0.00/2.00 (mean/median/min/max)  1046.89 steps/s\n",
      "Returns 0.26/0.00/0.00/3.00 (mean/median/min/max)  1050.84 steps/s\n",
      "Returns 0.32/0.00/0.00/3.00 (mean/median/min/max)  1073.85 steps/s\n",
      "Returns 0.32/0.00/0.00/3.00 (mean/median/min/max)  1079.37 steps/s\n",
      "Returns 0.20/0.00/0.00/2.00 (mean/median/min/max)  1037.17 steps/s\n",
      "Returns 0.26/0.00/0.00/2.00 (mean/median/min/max)  1004.75 steps/s\n",
      "Returns 0.28/0.00/0.00/2.00 (mean/median/min/max)  970.53 steps/s\n",
      "Returns 0.22/0.00/0.00/2.00 (mean/median/min/max)  1093.18 steps/s\n",
      "Returns 0.28/0.00/0.00/3.00 (mean/median/min/max)  993.45 steps/s\n",
      "Returns 0.44/0.00/0.00/3.00 (mean/median/min/max)  1029.09 steps/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f39684ba8557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_iter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorchrl/torchrl/a2c/a2c.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# collect rollout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mrollout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorchrl/torchrl/a2c/a2c.py\u001b[0m in \u001b[0;36mcollect_rollout\u001b[0;34m(self, num_rollout)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             actions, log_probs, entropys, values = self.model(\n\u001b[0;32m---> 47\u001b[0;31m                 self.phi(self.curr_states))\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorchrl/torchrl/a2c/a2c.py\u001b[0m in \u001b[0;36mphi\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorchrl/torchrl/utils/utils.py\u001b[0m in \u001b[0;36mv_wrap\u001b[0;34m(x, dtype)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mv_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mweights_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "model.train()\n",
    "t0 = time.time()\n",
    "\n",
    "log_interval = params[\"log_interval\"]\n",
    "log_iter = int(log_interval // batch_size)\n",
    "\n",
    "for e in range(num_iter):\n",
    "    agent.train_step()\n",
    "    \n",
    "    if e % log_iter == 0:\n",
    "        writer.add_scalar('policy loss', agent.policy_loss, agent.num_steps)\n",
    "        writer.add_scalar('entropy loss', agent.entropy_loss, agent.num_steps)\n",
    "        writer.add_scalar('value loss', agent.value_loss, agent.num_steps)\n",
    "        \n",
    "        if len(agent.episode_rewards) > 0:\n",
    "            writer.add_scalar('Mean Reward', np.mean(agent.episode_rewards), agent.num_steps)\n",
    "            writer.add_scalar('Max Reward', np.max(agent.episode_rewards), agent.num_steps)\n",
    "            \n",
    "            print('Returns %.2f/%.2f/%.2f/%.2f (mean/median/min/max)  %.2f steps/s' % (\n",
    "                np.mean(agent.episode_rewards),\n",
    "                np.median(agent.episode_rewards),\n",
    "                np.min(agent.episode_rewards),\n",
    "                np.max(agent.episode_rewards),\n",
    "                batch_size * log_iter / (time.time() - t0)\n",
    "            ))\n",
    "            \n",
    "            agent.episode_rewards.clear()\n",
    "\n",
    "            t0 = time.time()\n",
    "    \n",
    "    # save the model every 1000000 steps\n",
    "    if e % (1e6 // batch_size) == 0:\n",
    "        checkpoint = {\n",
    "            'num_steps': agent.num_steps,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict()\n",
    "            }\n",
    "        torch.save(checkpoint, \"Breakout/saved_model\" + str(agent.num_steps) + \".pth\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "temp_rollout = []\n",
    "for i in range(5):\n",
    "    log_probs = torch.randn(8)\n",
    "    values = torch.randn(8)\n",
    "    rewards = torch.randint(0, 2, (8,)).float()\n",
    "    terminals = torch.zeros((8,)).float()\n",
    "    entropys = torch.randn(8)\n",
    "    temp_rollout.append([log_probs, values, rewards, 1 - terminals, entropys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_value = torch.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4728])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437 µs ± 12 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "rollout = copy.deepcopy(temp_rollout)\n",
    "rollout.append([None, last_value, None, None, None])\n",
    "\n",
    "processed_rollout = [None] * (len(rollout) - 1)\n",
    "advantages = torch.zeros((8, 1))\n",
    "returns = last_value\n",
    "\n",
    "for i in reversed(range(len(rollout) - 1)):\n",
    "    log_prob, value, rewards, terminals, entropy = rollout[i]\n",
    "    next_value = rollout[i + 1][1]\n",
    "    returns = rewards + params[\"discount_gamma\"] * terminals * returns\n",
    "    \n",
    "    \n",
    "    td_error = rewards + params[\"discount_gamma\"] * terminals * next_value.detach() - value.detach()\n",
    "    advantages = advantages * params[\"gae_tau\"] * params[\"discount_gamma\"]  * terminals + td_error\n",
    "    \n",
    "    \n",
    "    processed_rollout[i] = [log_prob, value, returns, advantages, entropy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2792, 2.9892, 1.2792, 1.9353, 3.5643, 2.7182, 2.7182, 1.0892, 0.3102,\n",
       "        2.2102, 0.3102, 1.0392, 2.8492, 3.0202, 3.0202, 1.2102, 0.3447, 1.3447,\n",
       "        0.3447, 1.1547, 2.0547, 2.2447, 2.2447, 1.3447, 0.3830, 0.3830, 0.3830,\n",
       "        1.2830, 2.2830, 1.3830, 1.3830, 0.3830, 0.4256, 0.4256, 0.4256, 1.4256,\n",
       "        1.4256, 0.4256, 0.4256, 0.4256])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob, value, returns, advantages, entropy = map(lambda x: torch.cat(x, dim=0), zip(*processed_rollout))\n",
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 µs ± 1.28 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "rollout = copy.deepcopy(temp_rollout)\n",
    "returns = last_value\n",
    "discounted_rollout = []\n",
    "\n",
    "advantages = torch.zeros((8))\n",
    "\n",
    "for i in reversed(range(len(rollout))):\n",
    "    log_prob, values, rewards, terminals, entropy = rollout[i]\n",
    "    #values = values.squeeze()\n",
    "    returns = rewards + params[\"discount_gamma\"] * terminals * returns\n",
    "\n",
    "\n",
    "    next_values = last_value\n",
    "    gae_returns = rewards + params[\"discount_gamma\"] * terminals * next_values.detach()\n",
    "    td_error = gae_returns - values.detach()\n",
    "    advantages = advantages * params[\"gae_tau\"] * params[\"discount_gamma\"] * terminals + td_error\n",
    "\n",
    "    discounted_rollout.append(\n",
    "        [log_prob, values, returns, advantages, entropy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob, value, returns, advantages, entropy = map(lambda x: torch.cat(x, dim=0), zip(*processed_rollout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2792, 2.9892, 1.2792, 1.9353, 3.5643, 2.7182, 2.7182, 1.0892, 0.3102,\n",
       "        2.2102, 0.3102, 1.0392, 2.8492, 3.0202, 3.0202, 1.2102, 0.3447, 1.3447,\n",
       "        0.3447, 1.1547, 2.0547, 2.2447, 2.2447, 1.3447, 0.3830, 0.3830, 0.3830,\n",
       "        1.2830, 2.2830, 1.3830, 1.3830, 0.3830, 0.4256, 0.4256, 0.4256, 1.4256,\n",
       "        1.4256, 0.4256, 0.4256, 0.4256])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
