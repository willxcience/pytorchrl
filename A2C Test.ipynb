{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchrl\n",
    "import torchrl.network as network\n",
    "\n",
    "# later incorporate this into torchrl\n",
    "from rlutils.vec_env import SubprocVecEnv\n",
    "from rlutils.utils import make_atari_env\n",
    "from rlutils.vec_env import VecFrameStack\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env(\"BreakoutNoFrameskip-v4\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VecFrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"num_in_channels\"] = 4\n",
    "params[\"num_latent_nodes\"] = 512\n",
    "params[\"num_actions\"] = 4\n",
    "params[\"num_workers\"] = 4\n",
    "params[\"discount_gamma\"] = 0.9\n",
    "params[\"gae_tau\"] = 0.5\n",
    "params[\"use_gae\"] = True\n",
    "params[\"entropy_coef\"]\n",
    "params[\"value_coef\"]\n",
    "params[\"max_norm_grad\"]\n",
    "params[\"num_actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.e_coef = params[\"entropy_coef\"]\n",
    "        self.v_coef = params[\"value_coef\"]\n",
    "        self.max_grad_norm = params[\"max_norm_grad\"]\n",
    "        self.n_actions = params[\"num_actions\"]\n",
    "\n",
    "        # cnn head\n",
    "        self.cnn_head = network.NatureCNN(params)\n",
    "\n",
    "        # policy function\n",
    "        self.pf = nn.Linear(512, self.n_actions)\n",
    "        self.pdist = torch.distributions.Categorical\n",
    "        \n",
    "        # value function\n",
    "        self.vf = nn.Linear(512, 1)\n",
    "        \n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_head(x)\n",
    "        p = self.pf(x)\n",
    "        v = self.vf(x)\n",
    "        \n",
    "        pd = self.pdist(logits=p)\n",
    "        action = pd.sample()\n",
    "        log_prob = dist.log_prob(action).unsqueeze(-1)\n",
    "        \n",
    "        return action, log_prob, dist.entropy().unsqueeze(-1), v\n",
    "        \n",
    "    def value_func(self, x):\n",
    "        x = self.cnn_head(x)\n",
    "        v = self.vf(x)\n",
    "        return v\n",
    "        \n",
    "\n",
    "    def loss_func(self, rollout):\n",
    "\n",
    "        log_probs, values, returns, advantages, entropys = map(lambda x: torch.cat(x, dim=0), zip(*rollout))\n",
    "\n",
    "        policy_loss = (-advantages * log_probs).mean()\n",
    "        value_loss = self.mse(values, returns)\n",
    "\n",
    "        loss = policy_loss - entropys * self.e_coef + value_loss * self.v_coef\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_wrap(x, dtype=np.float32):\n",
    "    return torch.from_numpy(dtype(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "online_rewards = np.zeros(params[\"num_workers\"])\n",
    "num_workers = params[\"num_workers\"]\n",
    "gamma = params[\"discount_gamma\"]\n",
    "tau = params[\"gae_tau\"]\n",
    "use_gae = params[\"use_gae\"]\n",
    "\n",
    "curr_states = env.reset()\n",
    "\n",
    "# collect rollout used for training\n",
    "def collect_rollout(model, env, num_rollout=20):\n",
    "    \n",
    "    rollout = []\n",
    "\n",
    "    # predict and collect\n",
    "    for i in range(num_rollout):\n",
    "        # care if states are uint8\n",
    "        actions, log_probs, entropys, values = model(v_wrap(curr_states))\n",
    "\n",
    "        next_states, rewards, dones, _ = env.step(actions)\n",
    "        online_rewards += rewards\n",
    "        for i in range(dones.shape[0]):\n",
    "            if dones[i]:\n",
    "                episode_rewards.append(online_rewards[i])\n",
    "                online_rewards[i] = 0 \n",
    "        \n",
    "        rollout.append([actions, log_probs, entropys, values, rewards, 1 - dones])\n",
    "        curr_states = next_states\n",
    "    \n",
    "    \n",
    "    # calculate discounted returns and advantages\n",
    "    last_values = model.value_func(v_wrap(curr_states))\n",
    "    returns = last_values.detach()\n",
    "    advantages = torch.zeros((num_workers))\n",
    "    discounted_rollout = []\n",
    "    \n",
    "    for i in reversed(range(len(rollout) - 1)):\n",
    "        actions, log_probs, entropys, values, rewards, not_dones = rollout[i]\n",
    "        returns = rewards + gamma * not_dones * returns\n",
    "\n",
    "        if not use_gae:\n",
    "            advantages = returns - values.detach()\n",
    "        else:\n",
    "            next_values = last_values\n",
    "            gae_returns = rewards + gamma * not_dones * next_values.detach() \n",
    "            td_error = gae_returns - values.detach()\n",
    "            advantages = advantages * tau * gamma * not_dones + td_error\n",
    "    \n",
    "        discounted_rollout.append([log_probs, values, returns, advantages, entropys])\n",
    "    \n",
    "    return discounted_rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters, self.max_grad_norm)\n",
    "optimz.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
